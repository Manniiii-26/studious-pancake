[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.1 Project Goal",
    "text": "1.1 Project Goal\nThe objective of my project is to draw inferences and predictions from patients’ diagnostic measurements related to the detection of diabetes, with a focus on uncovering patterns or relationships between variables. My main goal is to understand the interactions between the various factors that we have available and to clarify which factors have a significant impact on diabetes outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#significance-of-the-project-question",
    "href": "index.html#significance-of-the-project-question",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.2 Significance of the Project Question",
    "text": "1.2 Significance of the Project Question\nThis topic is important to me because of the shocking statistics surrounding diabetes; approximately 38 million Americans have been diagnosed with diabetes, which is roughly 1 in 10 people. My objective is to investigate this issue in depth. I want to find out the key factors that contribute to the development of diabetes, with the ultimate goal of educating people about diabetes prevention and encouraging them to be active in monitoring their health before the condition gets worse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Context & Background Information\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The dataset has been collected with a sample size of 768 with 8 variables and a column of outcome. I got this dataset from Kaggle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#content",
    "href": "data.html#content",
    "title": "2  Data",
    "section": "2.2 Content",
    "text": "2.2 Content\n\nNumber of Sample: 768\nNumber of Attribute: 8\nPregnancies: number of times pregnant\nGlucose: plasma glucose concentration 2 hours in an oral glucose tolerance test\nBloodPressure: diastolic blood pressure (mm Hg)\nSkinThinkness: triceps skin fold thickness (mm)\nInsulin: 2-Hour serum insulin (mu U/ml)\nBMI: body mass index (weight in kg/(height in m)^2)\nDiabetesPedigreeFunction: diabetes pedigree function\nAge: age (years)\nOutcome: class variable (0 = No, 1 = Yes)\n\n\n\nCode\nlibrary(dplyr)\ndf &lt;- read.csv(\"data/diabetes.csv\")\nsummary(df)\n\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nCode\n# Summary of 0\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        5                       35 \n           SkinThickness                  Insulin                      BMI \n                     227                      374                       11 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.3 Missing value analysis",
    "text": "2.3 Missing value analysis\nThis dataset have missing values, they uses ‘0’ to fill in the dataset beforhand so I am not able to tell rather the ‘0’ means missing or the real response is actually zero. Expect for the ‘Outcome’ variable, ‘0’ indicate no diabeties.\nFrom the summary, variable ‘Pregnancies’ have 111 responses are ‘0’ which I will not mutate this column. Since this dataset is all females at least 21 years old, I do believe that some of the zeros for pregnanices does mean no pregnanices history.\nThere is 5 missing values for ‘Glucose’, 35 missing values for ‘BloodPressure’, 277 missing values for ‘SkinThickness’, 374 missing values for ‘Insulin’, 11 missing values for ‘BMI’, and no missing values for ‘DiabetesPedigreeFunction’ and ‘Age’. I would replace zeros with the median of the non-zero values, median is robust to outliers and is commonly used in medical datasets.\n\n\nCode\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        0                        0 \n           SkinThickness                  Insulin                      BMI \n                       0                        0                        0 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500 \n\n\nIn my study, the variable I propose to categorize and make inferences about “Outcome”, indicates the potential diagnosis of diabetes. This variable determines whether a diabetes test result is positive (category value 1) or negative (category value 0). A remarkable aspect of our dataset is the “pregnancy” variable, with a minimum value of 0, a maximum value of 17, and a mean value of 3.845. It would be particularly informative to explore whether pregnancy is an important factor influencing the outcome of a diabetes diagnosis and to assess the degree to which it does so. In addition, common predictors such as body mass index (BMI) and age are expected to play an important role in determining outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Statistical Technique",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#statistical-technique",
    "href": "methods.html#statistical-technique",
    "title": "3  Methods",
    "section": "",
    "text": "3.1.1 Tree Classification\n\nThe idea behind trees is to create multiple regions corresponding to different values (or intervals) of the predictors. Prediction is then conducted by assigning to a new observation the mean/median/mode (or maximum class probability) of the response computed over the region to which the new observation belongs to. Tree classification will also give us a meaningful visual representation of the variables.\n\n\n\n3.1.2 Random Forest\n\nRandom Forest, registered as a trademark by Leo Breiman and Adele Cutler. The algorithm’s ease of use and flexibility have contributed to its adoption since it can handle both classification and regression problems. It can help in identifying which features or variables are most important in making predictions. This is particularly useful for understanding the underlying patterns in the data and it also helps reduce the variance and gain uncorrelated trees.\n\n\n\n3.1.3 Partial Dependence Plot(PDP)\n\nGiven the project goal, PDP open up the black box and show how each variable affects the probability of diabetes, while holding other variables constant. PDP can shows marginal effect of one or two features on the predicted outcome, the marginal effect is about the effect of a feature on the model while holding other features constant. Provide actionable insights which allow interpretation that can help with education, prevention, and targeted interventions about our project.\n\n\n\n3.1.4 LIME\n\nLocal Interpretable Model-agnostic Explanations, starts by choosing a specific sample, x’, which is our data point of interest. It then perturbs this sample by creating fake data points in the neighborhood of x’. These fake data points are fed into the black box model to obtain predictions. Each fake data point is weighted based on its distance or similarity to x’, giving more importance to those closer to the original sample. After this, LIME fits a simple and interpretable model to these weighted fake data points. This local model is tuned as necessary and helps us observe how the black box model’s predictions change in response to variations around x’. Ultimately, this process reveals which features were most important for that specific prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "4  Results",
    "section": "",
    "text": "4.1 Prepare the data\nCode\n# Prepare the data\n\ndf &lt;- read.csv(\"data/diabetes.csv\")\ndf &lt;- df %&gt;%\n  mutate(Outcome = ifelse(Outcome == 0, \"No Diabetes\", \"Diabetes\"))\ndf$Outcome &lt;- as.factor(df$Outcome)\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n#sapply(df, function(x) sum(x == 0))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#prepare-the-data",
    "href": "results.html#prepare-the-data",
    "title": "4  Results",
    "section": "",
    "text": "The data is imported and cleaned. The outcome variable is recoded from numeric (0/1) into descriptive labels (“No Diabetes” and “Diabetes”), improving clarity in the analysis and visualization. All missing values replaced with median.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#decision-tree-classification-model",
    "href": "results.html#decision-tree-classification-model",
    "title": "4  Results",
    "section": "4.2 Decision Tree Classification Model",
    "text": "4.2 Decision Tree Classification Model\n\nThis code below implements a Decision Tree classification model to predict diabetes outcomes using patients’ diagnostic data. The dataset used consists of multiple diagnostic measurements and an outcome variable, which indicates whether a patient has diabetes (“Diabetes”) or not (“No Diabetes”). The task below divided the dataset into training (70%) and testing (30%) sets to allow model training and performance evaluation. A decision tree is built where the outcome variable is predicted based on all available predictors from this dataset.\n\n\n\nCode\nset.seed(5293)\n# Split the data into training and testing sets (70% train, 30% test)\ntrain_index &lt;- createDataPartition(df$Outcome, p = 0.7, list = FALSE)\n\ntrain_data &lt;- df[train_index, ]\ntest_data &lt;- df[-train_index, ]\n\n# Train Decision Tree model\ntree_model &lt;- rpart(Outcome ~ ., data = train_data, method = \"class\")\n\n# Plot the tree\nrpart.plot(tree_model, type = 2, fallen.leaves = TRUE, box.palette = 'BuOr')\n\n\n\n\n\n\n\n\n\nCode\n#, extra = 3 - Display the misclassification rate\n\n\nThe decision tree graph result illustrates the process through which the model makes predictions about diabetes outcomes. Each internal node represents a decision rule based on one of the predictors. The tree splits first on Glucose with a value of 144. This indicates that Glucose levels play the most significant role in predicting diabetes. Patients with Glucose below this threshold are more likely to be “No Diabetes,” while those above are more likely to be “Diabetes”. Variables BMI, DiabetesPedigreeFunction and Age are other important factors. Subsequent splits on these variables suggest that they also meaningfully contribute to the model’s decision process. For example, among patients with lower Glucose levels, those with lower BMI and lower DiabetesPedigreeFunction are classified as “No Diabetes” more often. One of the advantage of the tree is that its structure reflects interactions between variables. For example, even when Glucose is high, lower DiabetesPedigreeFunction and younger Age may reduce the probability of being classified as “Diabetes”. The ‘0.65’ in the first node indicates the proportion of sample in this node that belong to the predicted class which in here, it is ‘No Diabetes’. In other words, 0.35 of the sample in this node are ‘Diabetes’. The higher this number, the more confident the prediction. The ‘100%’ in the first node is the percentage of total samples in the entire dataset that fall into this node, it begins with the whole dataset or the training dataset and it starts with 100%.\n\n4.2.1 Make predictions on the test set - display result with confusion matrix\n\n\nCode\npredictions &lt;- predict(tree_model, test_data, type = \"class\")\nconfusion &lt;- confusionMatrix(predictions, test_data$Outcome)\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          54          25\n  No Diabetes       26         125\n                                         \n               Accuracy : 0.7783         \n                 95% CI : (0.719, 0.8302)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 2.232e-05      \n                                         \n                  Kappa : 0.5098         \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6750         \n            Specificity : 0.8333         \n         Pos Pred Value : 0.6835         \n         Neg Pred Value : 0.8278         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2348         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7542         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nCode\n# Accuracy\ncat(\"Accuracy:\", confusion$overall[\"Accuracy\"], \"\\n\")\n\n\nAccuracy: 0.7782609 \n\n\nFrom the confusion matrix, we may see that there are 54 cases of true positive, which mean the patient have diabetes and we correctly predicted they do have diabetes. There are 125 cases of true negatives, 25 cases of false positives and 26 cases of false negatives. We may also refer to the accuracy rate, it tells us how much the model correctly predicted the outcome, that is, about 78% of the time it makes correct prediction. The sensitivity measures the model’s ability to correctly identify patients who actually have diabetes. In this case, we prefer a higher sensitivity because it is more important to detect as many diabetes cases as possible and missing a diabetes diagnosis could be serious. The specificity measures how well the model identifies patients who do not have diabetes. Around 83% of ‘No Diabetes’ cases are correctly identifies, this model is relatively good to avoiding false alarms, and people would not want unnecessary treatment.\nAlthough this unpruned decision tree captures a large number of splits and thus perfectly fits the training data, it introduces issues related to overfitting(too complex), reduced interpretability, and poorer generalization to new patients. An unpruned tree sensitive to every small changes in the training data and that could lead to large changes in the tree structure. By pruning the tree to remove unnecessary splits, the model can focus on the most important predictors, maintain a simpler and clearer structure, and perform more reliably when applied to unseen data. This is especially important in a healthcare context, where models need to be both accurate and interpretable to support decision-making.\n\n\n4.2.2 Pruning and Improving Decision Tree Model - Plot cross validation error\n\n\nCode\nplotcp(tree_model, las = 1, upper = 'splits')\n\n\n\n\n\n\n\n\n\nPruning the tree by the complexity parameter (cp) plot is an important step for selecting the optimal size of a decision tree for balancing the model complexity and predictive performance, it controls the pruning. The plot above shows the relationship between the tree’s complexity (number of splits) and its cross validation error rate. At the lowest cp values (0.01), the error starts to flatten or even increase slightly, this suggest that adding more splits does not improve performance and this suggests overfitting starts here.\nThe cp plot suggests that a cp value around 0.014 to 0.011 minimizes cross-validation error without overfitting. By pruning the tree at this cp, we can achieve a simpler, more robust model that improves generalization to new data while maintaining strong performance in detecting diabetes cases. Next, I will generate a pruned tree with best cp value.\n\n\n4.2.3 For future improvements - prune the tess to the best cp value with have the lowest xerror (cross validation error)\n\n\nCode\nbest_cp &lt;- tree_model$cptable[which.min(tree_model$cptable[,\"xerror\"]),\"CP\"]\npruned_tree &lt;- prune(tree_model, cp = best_cp)\n# See the improve version of tree\nrpart.plot(pruned_tree, type = 2, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n\n\nThis pruned tree is overall simpler and has fewer nodes. A lot of the small and specific nodes are being removed. This plot is now easier to interpret, less focused to the noise in the training data and less likely to overfit. Variable ‘Glucose’ is still the first split, indicate that this is still the most important predictor same as the unpruned tree. Other important variables such as ‘DiabetesPedigreeFunction’, ‘BMI’, ‘Pregnancies’ and ‘Age’ are still retained, meaning these are the still the strongly related factors to predicting diabetes outcomes. Overall, pruning improves the decision tree’s generalization ability while preserving its predictive power and interpretability, it matches well with the project’s goal of creating an understandable and effective predictive model for diabetes diagnosis.\n\n\nCode\n# New prediction\npruned_preds &lt;- predict(pruned_tree, test_data, type = \"class\")\n\nconfusion_pruned &lt;- confusionMatrix(pruned_preds, test_data$Outcome)\nprint(confusion_pruned)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          55          24\n  No Diabetes       25         126\n                                         \n               Accuracy : 0.787          \n                 95% CI : (0.7283, 0.838)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 5.84e-06       \n                                         \n                  Kappa : 0.529          \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6875         \n            Specificity : 0.8400         \n         Pos Pred Value : 0.6962         \n         Neg Pred Value : 0.8344         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2391         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7637         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nThe accurary rate have a very slightly increase, it went from 0.7783 to 0.787, this means the pruned tree correctly classified nearly 79% in the test set. Sensitivity is now 0.6875 and this tells us the pruned tree correctly identified roughly 69% of true diabetes cases. Compared to the unpruned version, which had about 0.675, this is slightly improved and pruning did not sacrifice the ability to detect diabetes. Specificity is 0.84, the model correctly classified 84% of people without diabetes. This shows the pruned tree maintains very good performance in correctly classifying “No Diabetes” cases.\n\n\n4.2.4 Further Improve the performance by Cross Validation CP Tuning\n\n\nCode\n# Train with xval (default xval=10 for cross-validation)\ntree_model_cv &lt;- rpart(Outcome ~ ., data = train_data, method = \"class\", cp = 0.01)\n#printcp(tree_model_cv)\nbest_cp &lt;- tree_model_cv$cptable[which.min(tree_model_cv$cptable[,\"xerror\"]),\"CP\"] #0.01 best cp\npruned_tree_cv &lt;- prune(tree_model_cv, cp = best_cp)\nrpart.plot(pruned_tree_cv, type = 2, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ncv_pruned_preds &lt;- predict(tree_model_cv, test_data, type = \"class\")\nconfusion_cv_pruned &lt;- confusionMatrix(cv_pruned_preds, test_data$Outcome)\nprint(confusion_cv_pruned)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          54          25\n  No Diabetes       26         125\n                                         \n               Accuracy : 0.7783         \n                 95% CI : (0.719, 0.8302)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 2.232e-05      \n                                         \n                  Kappa : 0.5098         \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6750         \n            Specificity : 0.8333         \n         Pos Pred Value : 0.6835         \n         Neg Pred Value : 0.8278         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2348         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7542         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nThe code above applies cross-validation tuning to improve the performance of a decision tree. The rpart model is trained with xval = 10 (default), which means it automatically performs 10-fold cross-validation during the tree-growing process. The idea is to assess how well trees of various complexities (sizes) perform on unseen data.\nAfter building the initial tree, the model has a cptable recording the performance and error rate for trees of different sizes, that is, different cp values. Again, the best cp is selected where the cross-validation error (xerror) is the lowest. This means we find the most balanced point between underfitting and overfitting. As a result, no significance improvement.\n\n\n4.2.5 Visual in 2D - for Two Variables ‘Glucose’ and ‘BMI’\n\n\nCode\ndf_sub &lt;- df %&gt;% select(Glucose, BMI, Outcome)\nsimple_tree &lt;- rpart(Outcome ~ Glucose + BMI, data = df_sub, method = \"class\", cp = 0.01)\n\nglucose_range &lt;- seq(min(df_sub$Glucose), max(df_sub$Glucose), length.out = 200)\nbmi_range &lt;- seq(min(df_sub$BMI), max(df_sub$BMI), length.out = 200)\ngrid &lt;- expand.grid(Glucose = glucose_range, BMI = bmi_range)\n\ngrid$Outcome &lt;- predict(simple_tree, grid, type = \"class\")\n\n# Plot decision boundary with geom_tile (better color)\nggplot() +\n  geom_tile(data = grid, aes(x = Glucose, y = BMI, fill = Outcome), alpha = 0.5) +\n  geom_point(data = df_sub, aes(x = Glucose, y = BMI, color = Outcome), size = 2) +\n  labs(title = \"Decision Boundary (Glucose vs BMI)\", x = \"Glucose\", y = \"BMI\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis decision boundary plot contains variabls ‘Glucose’ and ‘BMI’, give us intuitive visualization of the model’s decision process. This plot shows exactly how the decision tree splits the data using simple rules. It makes the model interpretable, clear vertical separation shows ‘Glucose’ is the primary decision factor, and ‘BMI’ is limited horizontal influence shows it plays a secondary role. This helps explain and justify which variables are most useful in prediction. From this plot, we can easily spot where the model makes mistakes and roughly how accurate our model performs.\n\n\n4.2.6 Another Plot in 2D - Easier to Spot the Misclassification Points\n\n\nCode\ndf_twovar &lt;- df |&gt; \n  mutate(Glucose_std = scale(Glucose),\n         BMI_std = scale(BMI))\n\ntrain_data_sub &lt;- df_twovar[train_index, ]\ntest_data_sub &lt;- df_twovar[-train_index, ]\n\n# Train Decision Tree (simple for visualization)\nsimple_tree &lt;- rpart(Outcome ~ Glucose + BMI, data = train_data_sub, method = \"class\", cp = 0.01)\n\n# Predict on test data\ntest_data_sub$pred &lt;- predict(simple_tree, test_data_sub, type = \"class\")\n\n# Create correct/incorrect column\ntest_data_sub &lt;- test_data_sub %&gt;%\n  mutate(correct = ifelse(Outcome == pred, \"Test - Correct\", \"Test - Incorrect\"))\n\n# Combine with train data\ntrain_data_sub$set &lt;- \"train\"\ntest_data_sub$set &lt;- \"test\"\n\nplot_data &lt;- bind_rows(train_data_sub, test_data_sub)\n\n# Plot\nggplot(plot_data, aes(x = Glucose_std, y = BMI_std)) +\n  geom_point(data = filter(plot_data, set == \"train\"),\n             aes(color = Outcome),\n             size = 3, alpha = 0.5) +\n  \n  geom_point(data = filter(plot_data, set == \"test\", correct == \"Test - Correct\"),\n             aes(shape = correct, color = Outcome),\n             size = 4, stroke = 1.5) +\n  \n  geom_point(data = filter(plot_data, set == \"test\", correct == \"Test - Incorrect\"),\n             aes(shape = correct, color = Outcome),\n             size = 5, stroke = 1.5) +\n  \n  scale_shape_manual(values = c(\"Test - Incorrect\" = 4)) +\n  \n  labs(title = \"Decision Tree Prediction Results on Diabetes Data (Glucose vs BMI)\",\n       x = \"Standardized Glucose\",\n       y = \"Standardized BMI\",\n       shape = \"Prediction Result\",\n       color = \"True Outcome\") +\n  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis graph highlights the misclassifications of the decision tree model using Glucose and BMI to predict diabetes. While correct predictions are well-separated, many incorrect predictions occur in the overlapping middle regions, where Glucose and BMI alone are insufficient to distinguish between classes. Compared to the decision boundary plot, which shows how the model partitions the feature space, this plot provides clearer insight into individual misclassifications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#random-forest",
    "href": "results.html#random-forest",
    "title": "4  Results",
    "section": "4.3 Random Forest",
    "text": "4.3 Random Forest\nWe are now moving from decision tree to random forest. The decision tree is simply and easy to interpert, but it has high rish of overfitting, when there is only one tree it could be very sensitive to noise and even a tiny changes in out dataset, it may not preform well with complex dataset. On the other hand, random forest is more complex but it improves the performance, it has much lower risk of overfitting because it averaging many trees. It is an ensemble of tree and that make this method more robust and stable. Generally, it produces much higher accuracy in most real-world dataset and we will see how it performs below.\nMoving to Random Forest is a way to achieve better prediction performance and reduce model variance after pruning and testing single trees. It improves performance and generalization at the cost of losing a bit of interpretability.\n\n\nCode\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nrf_preds &lt;- predict(rf_model, test_data)\n\nconfusion_rf &lt;- confusionMatrix(rf_preds, test_data$Outcome)\nprint(confusion_rf)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          55          25\n  No Diabetes       25         125\n                                          \n               Accuracy : 0.7826          \n                 95% CI : (0.7236, 0.8341)\n    No Information Rate : 0.6522          \n    P-Value [Acc &gt; NIR] : 1.156e-05       \n                                          \n                  Kappa : 0.5208          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.6875          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.6875          \n         Neg Pred Value : 0.8333          \n             Prevalence : 0.3478          \n         Detection Rate : 0.2391          \n   Detection Prevalence : 0.3478          \n      Balanced Accuracy : 0.7604          \n                                          \n       'Positive' Class : Diabetes        \n                                          \n\n\nCode\n# Compare accuracy\ncat(\"Pruned Decision Tree Accuracy:\", confusion_pruned$overall[\"Accuracy\"], \"\\n\")\n\n\nPruned Decision Tree Accuracy: 0.7869565 \n\n\nCode\ncat(\"Random Forest Accuracy:\", confusion_rf$overall[\"Accuracy\"], \"\\n\")\n\n\nRandom Forest Accuracy: 0.7826087 \n\n\nAs we can see above, the accuracy rate is 0.77, sensitivity is 0.64, and specificity is 0.84. The overall performance is similar to decision tree. The sensitivity rate which is diabetes patients are correctly identified and a bit lower than tree while specificity is slightly better.\nThe improvement I expect from random forest is not strong here. By considering possible reasons like size of the dataset, or the variables itself provide clear decision boundarie, I think tree might have just handled as good as random forest. Although result not surperingly good, random forest itself is more stable",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#compare-decision-tree-and-random-forest",
    "href": "results.html#compare-decision-tree-and-random-forest",
    "title": "4  Results",
    "section": "4.4 Compare Decision Tree and Random Forest",
    "text": "4.4 Compare Decision Tree and Random Forest\n\n\nCode\n# Predict on test data\ntree_preds &lt;- predict(pruned_tree_cv, test_data, type = \"prob\")[,2]\nroc_tree &lt;- roc(test_data$Outcome, tree_preds)\n# Train Random Forest\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nrf_preds_prob &lt;- predict(rf_model, test_data, type = \"prob\")[,2]\nroc_rf &lt;- roc(test_data$Outcome, rf_preds_prob)\n\nplot(roc_tree, col = \"blue\", main = \"ROC Curve: Pruned Tree vs Random Forest\")\nlines(roc_rf, col = \"red\")\nlegend(\"bottomright\", legend = c(\n  paste0(\"Pruned Tree AUC = \", round(auc(roc_tree), 3)),\n  paste0(\"Random Forest AUC = \", round(auc(roc_rf), 3))\n), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nThe ROC curve comparison highlights the trade-off between interpretability and performance. While the pruned decision tree offers a simpler model which have 0.753 AUC values, the Random Forest significantly outperforms it in prediction accuracy and its AUC values is 0.838). In this project, where accurate prediction of diabetes is critical, the Random Forest is better than pruned tree. However, the pruned tree remains valuable for explaining decision rules and successfully finds out the key risk factors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#partial-dependence-plot",
    "href": "results.html#partial-dependence-plot",
    "title": "4  Results",
    "section": "4.5 Partial Dependence Plot",
    "text": "4.5 Partial Dependence Plot\nWhile Random Forest offers strong predictive power, PDP often described as a “black-box” model. This means we know it makes good predictions, but we do not know which variables are driving these predictions or how they are impacting the predicted outcome. Although metrics like accuracy, sensitivity, specificity, and ROC/AUC show overall model performance, they do not tell us what the model has learned about individual variables. This method matches perfectly with my project’s goal, to find out the key facters that lead to diabetes.\n\n\nCode\n# Train Random Forest model\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nvars &lt;- colnames(df)[colnames(df) != \"Outcome\"]\n\n\npdp_data &lt;- map_dfr(vars, function(v) {\n  pd &lt;- pdp::partial(rf_model, pred.var = v, prob = TRUE, grid.resolution = 20) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(variable = v)\n  names(pd)[1:2] &lt;- c(\"value\", \"yhat\")\n  return(pd)\n})\n\nggplot(pdp_data, aes(x = value, y = yhat)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~variable, scales = \"free_x\") +\n  labs(title = \"Partial Dependence Plots for All Variables\",\n       x = \"Value\", y = \"Predicted Probability (yhat)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nEach panel in the plot shows how the predicted probability of diabetes (yhat) changes when the value of a single predictor variable increases.\nAnd here is my main findings: - Glucose and BMI are clearly the most critical features, they are sharp and consistent increases in prediction probability which mean they are very important for diabetes prediction.\n\nAge, DiabetesPedigreeFunction, and Pregnancies, contribute but have moderate to mild effects.\nInsulin, BloodPressure, and SkinThickness are very flat lines meaning these variables have little effect on the prediction across their range, they are less important in the model’s decision making.\n\n\n4.5.1 Variable Importance - Confirm with PDP\nPartial Dependence Plots provided insight into how individual variables influence the predicted probability of diabetes, and it is also important to understand which variables contribute the most overall to the model’s predictive power. To achieve this, we use Variable Importance measures from the Random Forest model to quantify and rank the significance of each feature.\n\n\nCode\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\n\nimp &lt;- importance(rf_model, type = 2) \nimp_df &lt;- data.frame(\n  variable = rownames(imp),\n  importance = imp[, 1]\n)\nimp_df &lt;- imp_df %&gt;%\n  mutate(importance_scaled = importance / sum(importance) * 100) %&gt;%\n  arrange(importance_scaled)\n\nggplot(imp_df, aes(x = importance_scaled, y = reorder(variable, importance_scaled))) +\n  geom_point(size = 3) +\n  labs(title = \"Variable Importance (Scaled to Sum to 100)\",\n       x = \"Variable Importance Score\",\n       y = \"\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThe plot above summarizes which predictors have the greatest impact on the Random Forest model’s performance. The results show that Glucose is by far the most influential variable, followed by BMI, Age, and DiabetesPedigreeFunction. These variables likely play key roles in determining whether a patient has diabetes. Bottom down the list, variables like Pregnancies, SkinThickness, and BloodPressure have relatively lower importance, suggesting they are less sigificant in making predictions in this dataset.\n\n\n4.5.2 MeanDecreaseAccuracy vs MeanDecreaseGini\n\n\nCode\ndf2 &lt;- data.frame(rf_model$importance)\ndf2 |&gt; \n  rownames_to_column('feature') |&gt; \n  pivot_longer(cols = starts_with('Mean')) |&gt; \n  ggplot(aes(x = value, y = feature, color = name)) +\n  geom_point() +\n  theme_bw() +\n  theme(legend.position = 'top')\n\n\n\n\n\n\n\n\n\nThis plot shows two measures of variable importance calculated from the Random Forest model MeanDecreaseAccuracy which is the red dots, this measures how much the accuracy of the model decreases when the values of a variable are permuted. Higher values mean the variable is more important. The MeanDecreaseGini is the blue dots, this measures how much each variable contributes to decreasing the Gini impurity in the forest’s nodes. In simpler terms, it reflects how often and how well this variable was used to split data in decision trees.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#lime---local-interpretable-model-agnostic-explanations",
    "href": "results.html#lime---local-interpretable-model-agnostic-explanations",
    "title": "4  Results",
    "section": "4.6 LIME - Local Interpretable Model-Agnostic Explanations",
    "text": "4.6 LIME - Local Interpretable Model-Agnostic Explanations\nPartial Dependence Plots provided valuable insights into the global importance and average effect of each feature on diabetes prediction, they do not explain how individual predictions are made. To better understand local behavior for specific cases, I use LIME to helps us reveal the exact features driving each prediction on a case by case basis.\n\n\nCode\nset.seed(5293)\nn &lt;- nrow(df)\ntest_index &lt;- sample(n, 2)\ntrain_data &lt;- df[-test_index, ]\ntest_data &lt;- df[test_index, ]\n\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\n\nmodel_type.randomForest &lt;- function(x, ...) {\n  return(\"classification\")\n}\n\nexplainer &lt;- lime::lime(subset(train_data, select = -Outcome), rf_model)\nexplanation &lt;- lime::explain(x = subset(test_data, select = -Outcome),\n                             explainer = explainer, labels = 'No Diabetes', n_features = 7,\n                             n_permutations = 1000, feature_select = 'lasso_path')\n\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\nThe graph shows explanations for two individual predictions, the Case 339 and Case 11, where the model predicted “No Diabetes”. The bars indicate how much each feature contributed to the prediction. Red bars is Contradicts meaning features pushing the prediction away from “No Diabetes” and toward Diabetes. Blue bars is Supports, features pushing the prediction toward “No Diabetes”.\nFor case 339, the probability of “No Diabetes” is low which is 0.15, so LIME explains why the model leaned toward predicting Diabetes. The biggest negative impact comes from Glucose &gt; 140, which strongly pushes toward diabetes. Other negative contributors include Pregnancies &gt; 6, DiabetesPedigreeFunction &gt; 0.626 and Insulin &gt; 127. There is no supporting features therefore prediction probability for “No Diabetes” is very low. In short, this person have very high glucose and other risk factors made the model very likely to predict diabetes.\nFor case 11, the probability of “No Diabetes” is high which is 0.86, the strongest positive influences pushing toward “No Diabetes” include, BMI &lt;= 36.6, Glucose &lt;= 117 and Age &lt;= 41. There are some weak negative contributors (pushing toward diabetes), like Insulin &gt; 120 and SkinThickness &gt; 25. However, the positive signals dominate so the the model confidently predicts No Diabetes. This person, healthy glucose levels, younger age, and lower BMI greatly increased the likelihood of predicting “No Diabetes”, even though minor risk factors exist.\n\n4.6.1 Gower Plot\n\n\nCode\nnodiabetes &lt;- df %&gt;%\n  filter(Outcome == \"No Diabetes\")\n\nset.seed(123)\nindex &lt;- sample(nrow(nodiabetes), 10)\nnodiabetes_samp &lt;- nodiabetes[index, ]\n\n# Compute Gower distance matrix (excluding Outcome column)\ngower_dist &lt;- daisy(nodiabetes_samp[, -which(names(nodiabetes_samp) == \"Outcome\")])\n\n# Convert to dataframe for plotting\ndf_gower &lt;- as.data.frame(as.matrix(gower_dist)) %&gt;%\n  rownames_to_column('obs1') %&gt;%\n  pivot_longer(cols = -obs1, names_to = \"obs2\", values_to = \"gower\") %&gt;%\n  mutate(obs1 = as.numeric(obs1),\n         obs2 = as.numeric(obs2)) %&gt;%\n  filter(obs1 &lt; obs2)\n\n# Plot Gower distances\nggplot(df_gower, aes(x = gower)) +\n  geom_dotplot() +\n  labs(title = \"Gower Distances for Pairs of Survived Patients\",\n       x = \"Gower Distance\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis part calculates the Gower distance for pairs of patients who were classified as “No Diabetes” and visualizes the distances. The left cluster (0–100 Gower Distance) on the plot is showing most pairs have low Gower distances, meaning that many “No Diabetes” patients are quite similar to each other across all measured features. These similarities might be in features like glucose, age, BMI. It could indicate a consistent pattern in “No Diabetes” patients. The right Cluster (300–500 Gower Distance) is showing that some patient pairs have high Gower distances, suggesting they are much less similar. This indicate within “No Diabetes” patients, there is still heterogeneity, some people look very different based on their profile but still ended up being classified as “No Diabetes”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "5.1 Tree Classification\nThe Decision Tree analysis offers valuable insights into the diagnostic factors affecting diabetes outcomes. Through simple and interpretable rules, it highlights Glucose as the dominant predictor while also acknowledging the role of other variables. This supports the project’s objective of clarifying significant factors and uncovering meaningful patterns, which ultimately can help in guiding interventions and educational efforts focused on diabetes prevention and monitoring.\nThe model performs reasonably well overall, with good accuracy and specificity, meaning it is effective at identifying patients without diabetes. However, the sensitivity for diabetes cases is moderate, meaning some true diabetes cases were missed. In a healthcare context, where failing to detect diabetes can have serious consequences, it may be beneficial to optimize for higher sensitivity even if that lowers overall accuracy slightly. The confusion matrix shows that false positives and false negatives are both present, but false negatives (missed diabetes) are slightly more concerning.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#random-forest",
    "href": "conclusion.html#random-forest",
    "title": "5  Conclusion",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\nWhile Random Forest did not significantly outperform the pruned Decision Tree in terms of accuracy, it achieved slightly better specificity and balanced accuracy, indicating more stable and robust performance. Although sensitivity was modest, Random Forest still presents a valuable method in this analysis by providing a model less likely to overfit and better capable of handling more complex or noisy data. It matches with the project’s goals, Random Forest complements Decision Tree analysis by supporting reliable diabetes detection in addition to interpretable decision making.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#partial-dependence-plot",
    "href": "conclusion.html#partial-dependence-plot",
    "title": "5  Conclusion",
    "section": "5.3 Partial Dependence Plot",
    "text": "5.3 Partial Dependence Plot\nRandom Forest provided high predictive performance, but we needed PDP to understand which variables matter and how they influence predictions. PDP revealed that Glucose, followed by BMI and DiabetesPedigreeFunction, are the strongest contributors to higher diabetes risk predictions. This method goes particularly well along with my project goal because now, we are not only predicting diabetes but also able to clarify which factors have significant impacts, offering deeper insights into diabetes outcomes that can help inform prevention strategies.\nThe variable importance method confirms the findings in PDP, the plot that variable importance provided have the same ranking aligns with the trends observed in the PDP plots, where Glucose and BMI showed more dramatic shifts in predicted probabilities. Together, these analyses confirm the central role of Glucose and BMI in predicting diabetes outcomes, reinforcing their importance in any decision making or medical guidance derived from the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#lime",
    "href": "conclusion.html#lime",
    "title": "5  Conclusion",
    "section": "5.4 LIME",
    "text": "5.4 LIME\nLIME offers individual-level explanations, showing which features contribute to each single prediction. This complements PDP, it shows the global patterns and LIME shows the local, individualized explanations. LIME is very helpful for decision support, for example when explaining to doctors or patients why a certain prediction was made.By breaking down complex predictions from models like Random Forest, LIME makes the decision making process clearer and easier.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#overall",
    "href": "conclusion.html#overall",
    "title": "5  Conclusion",
    "section": "5.5 Overall",
    "text": "5.5 Overall\nIn this project, we explored and compared several machine learning approaches to predict diabetes and interpret the results in meaningful ways. Starting with a Decision Tree, we quickly learned that while easy to interpret, unpruned trees risk overfitting and losing generalizability. Pruning improved performance and stability, but still left some room for improvement in predictive power.\nMoving to Random Forest allowed us to achieve more robust and balanced predictions. Although the overall accuracy improvement over the pruned Decision Tree was modest, Random Forest offered clear advantages in terms of reducing overfitting and handling more complex patterns in the data. This aligns well with our project goal of building a model that not only predicts accurately, but also generalizes reliably.\nHowever, Random Forest’s complexity makes it less interpretable. To address this, we incorporated Partial Dependence Plots, which revealed which variables had the strongest overall influence on predictions. Glucose, BMI, and DiabetesPedigreeFunction stood out as the most impactful features. The PDPs helped bridge the gap between predictive performance and interpretability at the global, overall model level.\nWe further confirmed these findings using Variable Importance plots, which supported the same variable rankings seen in the PDP analysis. This reinforced the understanding that Glucose and BMI are consistently crucial predictors across various perspectives.\nFinally, to address the need for local interpretability, we introduced LIME. Unlike PDP and Variable Importance, which show global trends, LIME allowed us to zoom in on individual predictions and understand exactly why a model predicted a particular outcome for a specific patient. This is especially useful for real world applications, case by case explanations before making decisions. LIME helps us to understand by turning complex model decisions into clear, straightforward explanations that make sense to nontechnical audiences\nOverall, with all those approach combining predictive power with multiple forms of interpretability, allows to make accurate diabetes prediction and meaningful insights. Each method contributes uniquely, Decision Trees offer simplicity, Random Forest ensures robustness, PDP and Variable Importance provide global understanding, and LIME delivers case-level clarity. Together, they form a comprehensive and practical framework for both prediction and explanation in diabetes detection and risk assessment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]