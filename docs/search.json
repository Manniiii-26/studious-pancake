[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.1 Project Goal",
    "text": "1.1 Project Goal\nThe objective of my project is to draw inferences and predictions from patients’ diagnostic measurements related to the detection of diabetes, with a focus on uncovering patterns or relationships between variables. My main goal is to understand the interactions between various factors and clarify which ones have a significant impact on diabetes outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#significance-of-the-project-question",
    "href": "index.html#significance-of-the-project-question",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.2 Significance of the Project Question",
    "text": "1.2 Significance of the Project Question\nThis topic is important to me because of the shocking statistics surrounding diabetes, approximately 38 million Americans have been diagnosed with diabetes, which is roughly 1 in 10 people. My objective is to investigate this issue in depth. I want to find out the key factors that contribute to the development of diabetes, with the ultimate goal of educating people about diabetes prevention and encouraging them to be active in monitoring their health before the condition gets worse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Context & Background Information\nThis dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases. Several constraints were applied when selecting instances from a larger database. In particular, all patients are females at least 21 years old and of Pima Indian heritage. The dataset consists of 768 samples, with 8 variables and an outcome column. The dataset was obtained from Kaggle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#content",
    "href": "data.html#content",
    "title": "2  Data",
    "section": "2.2 Content",
    "text": "2.2 Content\n\nNumber of Sample: 768\nNumber of Attribute: 8\nPregnancies: number of times pregnant\nGlucose: plasma glucose concentration 2 hours in an oral glucose tolerance test\nBloodPressure: diastolic blood pressure (mm Hg)\nSkinThinkness: triceps skin fold thickness (mm)\nInsulin: 2-Hour serum insulin (mu U/ml)\nBMI: body mass index (weight in kg/(height in m)^2)\nDiabetesPedigreeFunction: diabetes pedigree function\nAge: age (years)\nOutcome: class variable (0 = No, 1 = Yes)\n\n\n\nCode\nlibrary(dplyr)\ndf &lt;- read.csv(\"data/diabetes.csv\")\nsummary(df)\n\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nCode\n# Summary of 0\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        5                       35 \n           SkinThickness                  Insulin                      BMI \n                     227                      374                       11 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.3 Missing value analysis",
    "text": "2.3 Missing value analysis\nThis dataset contains missing values, with ‘0’ used to fill them beforehand. As a result, it is difficult to determine whether a ‘0’ represents a missing value or an actual measurement of zero.\nFrom the summary, variable ‘Pregnancies’ have 111 responses are ‘0’ which I will not mutate this column. Since this dataset includes only females who are at least 21 years old, I believe that some of the zeros in the pregnancies variable likely indicate no pregnancy history.\nThere is 5 missing values for ‘Glucose’, 35 missing values for ‘BloodPressure’, 277 missing values for ‘SkinThickness’, 374 missing values for ‘Insulin’, 11 missing values for ‘BMI’, and no missing values for ‘DiabetesPedigreeFunction’ and ‘Age’. I would replace zeros with the median of the non-zero values. The median is robust to outliers and is commonly used in medical datasets.\n\n\nCode\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        0                        0 \n           SkinThickness                  Insulin                      BMI \n                       0                        0                        0 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500 \n\n\nIn my study, the variable I propose to categorize and make inferences about “Outcome”, indicates the potential diagnosis of diabetes. This variable determines whether a diabetes test result is positive (category value 1) or negative (category value 0). One notable aspect of our dataset is the “Pregnancies” variable, which has a minimum value of 0, a maximum value of 17, and a mean of 3.845. It would be particularly informative to explore whether pregnancy is an important factor influencing the outcome of a diabetes diagnosis and to assess the degree to which it does so. Additionally, common predictors such as body mass index (BMI) and age are also expected to play important roles in determining outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods - Statistical Technique",
    "section": "",
    "text": "3.1 Tree Classification\nThe idea behind trees is to create multiple regions corresponding to different values (or intervals) of the predictors. Prediction is then made by assigning a new observation the mean, median, mode, or maximum class probability of the response calculated for the region to which the observation belongs. Tree classification also provides a meaningful visual representation of the variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods - Statistical Technique</span>"
    ]
  },
  {
    "objectID": "methods.html#random-forest",
    "href": "methods.html#random-forest",
    "title": "3  Methods - Statistical Technique",
    "section": "3.2 Random Forest",
    "text": "3.2 Random Forest\nRandom Forest, developed by Leo Breiman and Adele Cutler, is a trademarked algorithm. Its ease of use and flexibility have contributed to its widespread adoption, as it can handle both classification and regression problems. It helps identify which features or variables are most important for making predictions. This is particularly useful for understanding underlying patterns in the data, and it also helps reduce variance and produce uncorrelated trees.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods - Statistical Technique</span>"
    ]
  },
  {
    "objectID": "methods.html#partial-dependence-plotpdp",
    "href": "methods.html#partial-dependence-plotpdp",
    "title": "3  Methods - Statistical Technique",
    "section": "3.3 Partial Dependence Plot(PDP)",
    "text": "3.3 Partial Dependence Plot(PDP)\nGiven the project goal, PDP helps open up the black box and shows how each variable affects the probability of diabetes, while holding other variables constant. PDP can show the marginal effect of one or two features on the predicted outcome. This marginal effect reflects how much a specific feature impacts the model while all other features are held constant. PDP provides actionable insights that aid in interpretation, helping with education, prevention, and targeted interventions in our project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods - Statistical Technique</span>"
    ]
  },
  {
    "objectID": "methods.html#lime",
    "href": "methods.html#lime",
    "title": "3  Methods - Statistical Technique",
    "section": "3.4 LIME",
    "text": "3.4 LIME\nLocal Interpretable Model-agnostic Explanations (LIME) starts by choosing a specific sample, x’, which serves as the data point of interest. It then perturbs this sample by creating fake data points in the neighborhood of x’. These synthetic data points are fed into the black box model to generate predictions. Each point is weighted based on its distance or similarity to x’, giving more importance to those closer to the original sample. LIME then fits a simple and interpretable model to these weighted points. This local model helps us understand how the black box model’s predictions change in response to variations near x’. Ultimately, this process highlights which features are most important for that specific prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods - Statistical Technique</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "4  Results",
    "section": "",
    "text": "4.1 Prepare the data\nThe data is imported and cleaned. The outcome variable is recoded from numeric (0/1) into descriptive labels (“No Diabetes” and “Diabetes”), improving clarity in the analysis and visualization. All missing values replaced with median.\nCode\n# Prepare the data\n\ndf &lt;- read.csv(\"data/diabetes.csv\")\ndf &lt;- df %&gt;%\n  mutate(Outcome = ifelse(Outcome == 0, \"No Diabetes\", \"Diabetes\"))\ndf$Outcome &lt;- as.factor(df$Outcome)\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n#sapply(df, function(x) sum(x == 0))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#decision-tree-classification-model",
    "href": "results.html#decision-tree-classification-model",
    "title": "4  Results",
    "section": "4.2 Decision Tree Classification Model",
    "text": "4.2 Decision Tree Classification Model\nThe code below implements a Decision Tree classification model to predict diabetes outcomes using patients’ diagnostic data. The dataset consists of multiple diagnostic measurements and an outcome variable, which indicates whether a patient has diabetes or not. The dataset is split into training (70%) and testing (30%) sets to allow for model training and performance evaluation. A decision tree is then built to predict the outcome based on all available predictors from this dataset.\n\n\nCode\nset.seed(5293)\ntrain_index &lt;- createDataPartition(df$Outcome, p = 0.7, list = FALSE)\n\ntrain_data &lt;- df[train_index, ]\ntest_data &lt;- df[-train_index, ]\n\ntree_model &lt;- rpart(Outcome ~ ., data = train_data, method = \"class\")\nrpart.plot(tree_model, type = 2, fallen.leaves = TRUE, box.palette = 'BuOr')\n\n\n\n\n\n\n\n\n\nThe decision tree graph illustrates the process by which the model makes predictions about diabetes outcomes. Each internal node represents a decision rule based on one of the predictors. The tree splits first on Glucose with a value of 144, indicating that glucose levels play the most significant role in predicting diabetes. Patients with glucose levels below this threshold are more likely to be classified as “No Diabetes”, while those above are more likely to be classified as “Diabetes”. Further splits occur on other important variables, such as BMI, DiabetesPedigreeFunction, and Age, which also contribute meaningfully to the decision-making process. For example, patients with lower glucose levels, lower BMI, and lower DiabetesPedigreeFunction values are more often classified as “No Diabetes”. One of the key advantages of the decision tree is that it reflects interactions between variables. Even when glucose levels are high, lower DiabetesPedigreeFunction values or younger age may reduce the probability of being classified as “Diabetes”. Finally, the ‘0.65’ in the first node indicates the proportion of samples classified as “No Diabetes” at that point in the tree. The tree starts at the top with 100% of the dataset, which gradually splits based on predictor values, showing the pathway that leads to each prediction.\n\n4.2.1 Make predictions on the test set - display result with confusion matrix\n\n\nCode\npredictions &lt;- predict(tree_model, test_data, type = \"class\")\nconfusion &lt;- confusionMatrix(predictions, test_data$Outcome)\nprint(confusion)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          54          25\n  No Diabetes       26         125\n                                         \n               Accuracy : 0.7783         \n                 95% CI : (0.719, 0.8302)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 2.232e-05      \n                                         \n                  Kappa : 0.5098         \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6750         \n            Specificity : 0.8333         \n         Pos Pred Value : 0.6835         \n         Neg Pred Value : 0.8278         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2348         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7542         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nCode\n# Accuracy\ncat(\"Accuracy:\", confusion$overall[\"Accuracy\"], \"\\n\")\n\n\nAccuracy: 0.7782609 \n\n\nFrom the confusion matrix, we can see that there are 54 cases of true positives, meaning that the patients had diabetes and were correctly predicted as having diabetes. There are also 125 true negatives, along with 25 false positives and 26 false negatives. The accuracy rate shows us how often the model makes correct predictions — in this case, about 78% of the time. Sensitivity reflects the model’s ability to correctly identify patients who actually have diabetes. Since missing a diabetes diagnosis could have serious consequences, higher sensitivity is generally preferred in this context. Specificity, on the other hand, measures the model’s ability to correctly identify patients without diabetes. Here, around 83% of ‘No Diabetes’ cases are correctly identified, which suggests the model is also effective at avoiding false alarms — an important consideration, as people without diabetes should not receive unnecessary treatment.\nAlthough this unpruned decision tree captures many splits and fits the training data well, it introduces several challenges. These include overfitting (due to excessive complexity), reduced interpretability, and poorer generalization to new patients. Because an unpruned tree is highly sensitive to minor variations in the data, its structure may change drastically with small changes in input, potentially reducing its reliability. To address this, pruning helps remove unnecessary splits, allowing the model to focus on the most important predictors. This leads to a simpler and clearer structure, which performs better on unseen data. This is particularly important in a healthcare context, where models should be both accurate and interpretable to support decision-making effectively.\n\n\n4.2.2 Pruning and Improving Decision Tree Model - Plot cross validation error\n\n\nCode\nplotcp(tree_model, las = 1, upper = 'splits')\n\n\n\n\n\n\n\n\n\nPruning the tree by the complexity parameter (cp) plot is an important step for selecting the optimal size of a decision tree for balancing the model complexity and predictive performance, it controls the pruning. The plot above shows the relationship between the tree’s complexity (number of splits) and its cross validation error rate. At the lowest cp values (0.01), the error starts to flatten or even increase slightly, this suggest that adding more splits does not improve performance and this suggests overfitting starts here.\nThe cp plot suggests that a cp value around 0.014 to 0.011 minimizes cross-validation error without overfitting. By pruning the tree at this cp, we can achieve a simpler, more robust model that improves generalization to new data while maintaining strong performance in detecting diabetes cases. Next, I will generate a pruned tree with best cp value.\n\n\n4.2.3 For future improvements - prune the tess to the best cp value with have the lowest xerror (cross validation error)\n\n\nCode\nbest_cp &lt;- tree_model$cptable[which.min(tree_model$cptable[,\"xerror\"]),\"CP\"]\npruned_tree &lt;- prune(tree_model, cp = best_cp)\n# See the improve version of tree\nrpart.plot(pruned_tree, type = 2, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n\n\nThis pruned tree is overall simpler and has fewer nodes. A lot of the small and specific nodes are being removed. This plot is now easier to interpret, less focused to the noise in the training data and less likely to overfit. Variable ‘Glucose’ is still the first split, indicate that this is still the most important predictor same as the unpruned tree. Other important variables such as ‘DiabetesPedigreeFunction’, ‘BMI’, ‘Pregnancies’ and ‘Age’ are still retained, meaning these are the still the strongly related factors to predicting diabetes outcomes. Overall, pruning improves the decision tree’s generalization ability while preserving its predictive power and interpretability, it matches well with the project’s goal of creating an understandable and effective predictive model for diabetes diagnosis.\n\n\nCode\n# New prediction\npruned_preds &lt;- predict(pruned_tree, test_data, type = \"class\")\n\nconfusion_pruned &lt;- confusionMatrix(pruned_preds, test_data$Outcome)\nprint(confusion_pruned)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          55          24\n  No Diabetes       25         126\n                                         \n               Accuracy : 0.787          \n                 95% CI : (0.7283, 0.838)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 5.84e-06       \n                                         \n                  Kappa : 0.529          \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6875         \n            Specificity : 0.8400         \n         Pos Pred Value : 0.6962         \n         Neg Pred Value : 0.8344         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2391         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7637         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nThe accurary rate have a very slightly increase, it went from 0.7783 to 0.787, this means the pruned tree correctly classified nearly 79% in the test set. Sensitivity is now 0.6875 and this tells us the pruned tree correctly identified roughly 69% of true diabetes cases. Compared to the unpruned version, which had about 0.675, this is slightly improved and pruning did not sacrifice the ability to detect diabetes. Specificity is 0.84, the model correctly classified 84% of people without diabetes. This shows the pruned tree maintains very good performance in correctly classifying “No Diabetes” cases.\n\n\n4.2.4 Further Improve the performance by Cross Validation CP Tuning\n\n\nCode\n# Train with xval (default xval=10 for cross-validation)\ntree_model_cv &lt;- rpart(Outcome ~ ., data = train_data, method = \"class\", cp = 0.01)\n#printcp(tree_model_cv)\nbest_cp &lt;- tree_model_cv$cptable[which.min(tree_model_cv$cptable[,\"xerror\"]),\"CP\"] #0.01 best cp\npruned_tree_cv &lt;- prune(tree_model_cv, cp = best_cp)\nrpart.plot(pruned_tree_cv, type = 2, fallen.leaves = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ncv_pruned_preds &lt;- predict(tree_model_cv, test_data, type = \"class\")\nconfusion_cv_pruned &lt;- confusionMatrix(cv_pruned_preds, test_data$Outcome)\nprint(confusion_cv_pruned)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          54          25\n  No Diabetes       26         125\n                                         \n               Accuracy : 0.7783         \n                 95% CI : (0.719, 0.8302)\n    No Information Rate : 0.6522         \n    P-Value [Acc &gt; NIR] : 2.232e-05      \n                                         \n                  Kappa : 0.5098         \n                                         \n Mcnemar's Test P-Value : 1              \n                                         \n            Sensitivity : 0.6750         \n            Specificity : 0.8333         \n         Pos Pred Value : 0.6835         \n         Neg Pred Value : 0.8278         \n             Prevalence : 0.3478         \n         Detection Rate : 0.2348         \n   Detection Prevalence : 0.3435         \n      Balanced Accuracy : 0.7542         \n                                         \n       'Positive' Class : Diabetes       \n                                         \n\n\nThe code above applies cross-validation tuning to improve the performance of a decision tree. The rpart model is trained with xval = 10 (default), which means it automatically performs 10-fold cross-validation during the tree-growing process. The idea is to assess how well trees of various complexities (sizes) perform on unseen data.\nAfter building the initial tree, the model has a cptable recording the performance and error rate for trees of different sizes, that is, different cp values. Again, the best cp is selected where the cross-validation error (xerror) is the lowest. This means we find the most balanced point between underfitting and overfitting. As a result, no significance improvement.\n\n\n4.2.5 Visual in 2D - for Two Variables ‘Glucose’ and ‘BMI’\n\n\nCode\ndf_sub &lt;- df %&gt;% select(Glucose, BMI, Outcome)\nsimple_tree &lt;- rpart(Outcome ~ Glucose + BMI, data = df_sub, method = \"class\", cp = 0.01)\n\nglucose_range &lt;- seq(min(df_sub$Glucose), max(df_sub$Glucose), length.out = 200)\nbmi_range &lt;- seq(min(df_sub$BMI), max(df_sub$BMI), length.out = 200)\ngrid &lt;- expand.grid(Glucose = glucose_range, BMI = bmi_range)\n\ngrid$Outcome &lt;- predict(simple_tree, grid, type = \"class\")\n\n# Plot decision boundary with geom_tile (better color)\nggplot() +\n  geom_tile(data = grid, aes(x = Glucose, y = BMI, fill = Outcome), alpha = 0.5) +\n  geom_point(data = df_sub, aes(x = Glucose, y = BMI, color = Outcome), size = 2) +\n  labs(title = \"Decision Boundary (Glucose vs BMI)\", x = \"Glucose\", y = \"BMI\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis decision boundary plot contains variabls ‘Glucose’ and ‘BMI’, give us intuitive visualization of the model’s decision process. This plot shows exactly how the decision tree splits the data using simple rules. It makes the model interpretable, clear vertical separation shows ‘Glucose’ is the primary decision factor, while the limited horizontal influence of ‘BMI’ indicates it plays a secondary role. This helps explain and justify which variables are most useful in prediction. From this plot, we can easily spot where the model makes mistakes and roughly how accurate our model performs.\n\n\n4.2.6 Another Plot in 2D - Easier to Spot the Misclassification Points\n\n\nCode\ndf_twovar &lt;- df |&gt; \n  mutate(Glucose_std = scale(Glucose),\n         BMI_std = scale(BMI))\n\ntrain_data_sub &lt;- df_twovar[train_index, ]\ntest_data_sub &lt;- df_twovar[-train_index, ]\n\n# Train Decision Tree (simple for visualization)\nsimple_tree &lt;- rpart(Outcome ~ Glucose + BMI, data = train_data_sub, method = \"class\", cp = 0.01)\n\n# Predict on test data\ntest_data_sub$pred &lt;- predict(simple_tree, test_data_sub, type = \"class\")\n\n# Create correct/incorrect column\ntest_data_sub &lt;- test_data_sub %&gt;%\n  mutate(correct = ifelse(Outcome == pred, \"Test - Correct\", \"Test - Incorrect\"))\n\n# Combine with train data\ntrain_data_sub$set &lt;- \"train\"\ntest_data_sub$set &lt;- \"test\"\n\nplot_data &lt;- bind_rows(train_data_sub, test_data_sub)\n\n# Plot\nggplot(plot_data, aes(x = Glucose_std, y = BMI_std)) +\n  geom_point(data = filter(plot_data, set == \"train\"),\n             aes(color = Outcome),\n             size = 3, alpha = 0.5) +\n  \n  geom_point(data = filter(plot_data, set == \"test\", correct == \"Test - Correct\"),\n             aes(shape = correct, color = Outcome),\n             size = 4, stroke = 1.5) +\n  \n  geom_point(data = filter(plot_data, set == \"test\", correct == \"Test - Incorrect\"),\n             aes(shape = correct, color = Outcome),\n             size = 5, stroke = 1.5) +\n  \n  scale_shape_manual(values = c(\"Test - Incorrect\" = 4)) +\n  \n  labs(title = \"Decision Tree Prediction Results on Diabetes Data (Glucose vs BMI)\",\n       x = \"Standardized Glucose\",\n       y = \"Standardized BMI\",\n       shape = \"Prediction Result\",\n       color = \"True Outcome\") +\n  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis graph highlights the misclassifications of the decision tree model using Glucose and BMI to predict diabetes. While correct predictions are well-separated, many incorrect predictions occur in the overlapping middle regions, where Glucose and BMI alone are insufficient to distinguish between classes. Compared to the decision boundary plot, which shows how the model partitions the feature space, this plot provides clearer insight into individual misclassifications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#random-forest",
    "href": "results.html#random-forest",
    "title": "4  Results",
    "section": "4.3 Random Forest",
    "text": "4.3 Random Forest\nWe are now moving from decision tree to random forest. The decision tree is simply and easy to interpert, but it has high rish of overfitting, when there is only one tree it could be very sensitive to noise and even a tiny changes in out dataset, it may not preform well with complex dataset. On the other hand, random forest is more complex but it improves the performance, it has much lower risk of overfitting because it averaging many trees. It is an ensemble of tree and that make this method more robust and stable. Generally, it produces much higher accuracy in most real-world dataset and we will see how it performs below.\nMoving to Random Forest is a way to achieve better prediction performance and reduce model variance after pruning and testing single trees. It improves performance and generalization at the cost of losing a bit of interpretability.\n\n\nCode\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nrf_preds &lt;- predict(rf_model, test_data)\n\nconfusion_rf &lt;- confusionMatrix(rf_preds, test_data$Outcome)\nprint(confusion_rf)\n\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    Diabetes No Diabetes\n  Diabetes          55          25\n  No Diabetes       25         125\n                                          \n               Accuracy : 0.7826          \n                 95% CI : (0.7236, 0.8341)\n    No Information Rate : 0.6522          \n    P-Value [Acc &gt; NIR] : 1.156e-05       \n                                          \n                  Kappa : 0.5208          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.6875          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.6875          \n         Neg Pred Value : 0.8333          \n             Prevalence : 0.3478          \n         Detection Rate : 0.2391          \n   Detection Prevalence : 0.3478          \n      Balanced Accuracy : 0.7604          \n                                          \n       'Positive' Class : Diabetes        \n                                          \n\n\nCode\n# Compare accuracy\ncat(\"Pruned Decision Tree Accuracy:\", confusion_pruned$overall[\"Accuracy\"], \"\\n\")\n\n\nPruned Decision Tree Accuracy: 0.7869565 \n\n\nCode\ncat(\"Random Forest Accuracy:\", confusion_rf$overall[\"Accuracy\"], \"\\n\")\n\n\nRandom Forest Accuracy: 0.7826087 \n\n\nAs we can see above, the accuracy rate is 0.77, sensitivity is 0.64, and specificity is 0.84. The overall performance is similar to decision tree. The sensitivity rate which is diabetes patients are correctly identified and a bit lower than tree while specificity is slightly better.\nThe improvement I expect from random forest is not strong here. By considering possible reasons like size of the dataset, or the variables itself provide clear decision boundaries, I think tree might have just handled as good as random forest. Although the result is not surprisingly good, random forest itself is more stable",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#compare-decision-tree-and-random-forest",
    "href": "results.html#compare-decision-tree-and-random-forest",
    "title": "4  Results",
    "section": "4.4 Compare Decision Tree and Random Forest",
    "text": "4.4 Compare Decision Tree and Random Forest\n\n\nCode\n# Predict on test data\ntree_preds &lt;- predict(pruned_tree_cv, test_data, type = \"prob\")[,2]\nroc_tree &lt;- roc(test_data$Outcome, tree_preds)\n# Train Random Forest\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nrf_preds_prob &lt;- predict(rf_model, test_data, type = \"prob\")[,2]\nroc_rf &lt;- roc(test_data$Outcome, rf_preds_prob)\n\nplot(roc_tree, col = \"blue\", main = \"ROC Curve: Pruned Tree vs Random Forest\")\nlines(roc_rf, col = \"red\")\nlegend(\"bottomright\", legend = c(\n  paste0(\"Pruned Tree AUC = \", round(auc(roc_tree), 3)),\n  paste0(\"Random Forest AUC = \", round(auc(roc_rf), 3))\n), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nThe ROC curve comparison highlights the trade-off between interpretability and performance. While the pruned decision tree offers a simpler model which has an AUC value of 0.753, the Random Forest significantly outperforms it in prediction accuracy and its AUC values is 0.838). In this project, where accurate prediction of diabetes is critical, the Random Forest is better than pruned tree. However, the pruned tree remains valuable for explaining decision rules and successfully finds out the key risk factors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#partial-dependence-plot",
    "href": "results.html#partial-dependence-plot",
    "title": "4  Results",
    "section": "4.5 Partial Dependence Plot",
    "text": "4.5 Partial Dependence Plot\nWhile Random Forest offers strong predictive power, PDP often described as a “black-box” model. This means we know it makes good predictions, but we do not know which variables are driving these predictions or how they are impacting the predicted outcome. Although metrics like accuracy, sensitivity, specificity, and ROC/AUC show overall model performance, they do not tell us what the model has learned about individual variables. This method matches perfectly with my project’s goal, to find out the key factors that lead to diabetes.\n\n\nCode\n# Train Random Forest model\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\nvars &lt;- colnames(df)[colnames(df) != \"Outcome\"]\n\n\npdp_data &lt;- map_dfr(vars, function(v) {\n  pd &lt;- pdp::partial(rf_model, pred.var = v, prob = TRUE, grid.resolution = 20) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(variable = v)\n  names(pd)[1:2] &lt;- c(\"value\", \"yhat\")\n  return(pd)\n})\n\nggplot(pdp_data, aes(x = value, y = yhat)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~variable, scales = \"free_x\") +\n  labs(title = \"Partial Dependence Plots for All Variables\",\n       x = \"Value\", y = \"Predicted Probability (yhat)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nEach panel in the plot shows how the predicted probability of diabetes (yhat) changes when the value of a single predictor variable increases.\nmy main finding is:\n\nGlucose and BMI are clearly the most critical features, they are sharp and consistent increases in prediction probability which mean they are very important for diabetes prediction.\nAge, DiabetesPedigreeFunction, and Pregnancies, contribute but have moderate to mild effects.\nInsulin, BloodPressure, and SkinThickness are very flat lines meaning these variables have little effect on the prediction across their range, they are less important in the model’s decision making.\n\n\n4.5.1 Variable Importance - Confirm with PDP\nPartial Dependence Plots provided insight into how individual variables influence the predicted probability of diabetes, and it is also important to understand which variables contribute the most overall to the model’s predictive power. To achieve this, we use Variable Importance measures from the Random Forest model to quantify and rank the significance of each feature.\n\n\nCode\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\n\nimp &lt;- importance(rf_model, type = 2) \nimp_df &lt;- data.frame(\n  variable = rownames(imp),\n  importance = imp[, 1]\n)\nimp_df &lt;- imp_df %&gt;%\n  mutate(importance_scaled = importance / sum(importance) * 100) %&gt;%\n  arrange(importance_scaled)\n\nggplot(imp_df, aes(x = importance_scaled, y = reorder(variable, importance_scaled))) +\n  geom_point(size = 3) +\n  labs(title = \"Variable Importance (Scaled to Sum to 100)\",\n       x = \"Variable Importance Score\",\n       y = \"\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThe plot above summarizes which predictors have the greatest impact on the Random Forest model’s performance. The results show that Glucose is by far the most influential variable, followed by BMI, Age, and DiabetesPedigreeFunction. These variables likely play key roles in determining whether a patient has diabetes. Towards the bottom of the list, variables like Pregnancies, SkinThickness, and BloodPressure have relatively lower importance, suggesting they are less sigificant in making predictions in this dataset.\n\n\n4.5.2 MeanDecreaseAccuracy vs MeanDecreaseGini\n\n\nCode\ndf2 &lt;- data.frame(rf_model$importance)\ndf2 |&gt; \n  rownames_to_column('feature') |&gt; \n  pivot_longer(cols = starts_with('Mean')) |&gt; \n  ggplot(aes(x = value, y = feature, color = name)) +\n  geom_point() +\n  theme_bw() +\n  theme(legend.position = 'top')\n\n\n\n\n\n\n\n\n\nThis plot shows two measures of variable importance calculated from the Random Forest model MeanDecreaseAccuracy which is the red dots, this measures how much the accuracy of the model decreases when the values of a variable are permuted. Higher values mean the variable is more important. The MeanDecreaseGini is the blue dots, this measures how much each variable contributes to decreasing the Gini impurity in the forest’s nodes. In simpler terms, it shows how often and effectively each variable was used to split data in decision trees.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#lime---local-interpretable-model-agnostic-explanations",
    "href": "results.html#lime---local-interpretable-model-agnostic-explanations",
    "title": "4  Results",
    "section": "4.6 LIME - Local Interpretable Model-Agnostic Explanations",
    "text": "4.6 LIME - Local Interpretable Model-Agnostic Explanations\nPartial Dependence Plots provided valuable insights into the global importance and average effect of each feature on diabetes prediction, they do not explain how individual predictions are made. To better understand local behavior for specific cases, I use LIME to help us reveal the exact features driving each prediction on a case by case basis.\n\n\nCode\nset.seed(5293)\nn &lt;- nrow(df)\ntest_index &lt;- sample(n, 2)\ntrain_data &lt;- df[-test_index, ]\ntest_data &lt;- df[test_index, ]\n\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\n\nmodel_type.randomForest &lt;- function(x, ...) {\n  return(\"classification\")\n}\n\nexplainer &lt;- lime::lime(subset(train_data, select = -Outcome), rf_model)\nexplanation &lt;- lime::explain(x = subset(test_data, select = -Outcome),\n                             explainer = explainer, labels = 'No Diabetes', n_features = 7,\n                             n_permutations = 1000, feature_select = 'lasso_path')\n\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\nThe graph shows explanations for two individual predictions, the Case 339 and Case 11, where the model predicted “No Diabetes”. The bars indicate how much each feature contributed to the prediction. Red bars is Contradicts meaning features pushing the prediction away from “No Diabetes” and toward Diabetes. Blue bars is Supports, features pushing the prediction toward “No Diabetes”.\nFor case 339, the probability of “No Diabetes” is low which is 0.15, so LIME explains why the model leaned toward predicting Diabetes. The biggest negative impact comes from Glucose &gt; 140, which strongly pushes toward diabetes. Other negative contributors include Pregnancies &gt; 6, DiabetesPedigreeFunction &gt; 0.626 and Insulin &gt; 127. There is no supporting features therefore prediction probability for “No Diabetes” is very low. In short, this person has very high glucose and other risk factors made the model very likely to predict diabetes.\nFor case 11, the probability of “No Diabetes” is high which is 0.86, the strongest positive influences pushing toward “No Diabetes” include, BMI &lt;= 36.6, Glucose &lt;= 117 and Age &lt;= 41. There are some weak negative contributors (pushing toward diabetes), like Insulin &gt; 120 and SkinThickness &gt; 25. However, the positive signals dominate so the the model confidently predicts No Diabetes. This person, healthy glucose levels, younger age, and lower BMI greatly increased the likelihood that this person would be classified as ‘No Diabetes’, even though minor risk factors exist.\n\n4.6.1 Gower Plot\n\n\nCode\nnodiabetes &lt;- df %&gt;%\n  filter(Outcome == \"No Diabetes\")\n\nset.seed(123)\nindex &lt;- sample(nrow(nodiabetes), 10)\nnodiabetes_samp &lt;- nodiabetes[index, ]\n\n# Compute Gower distance matrix (excluding Outcome column)\ngower_dist &lt;- daisy(nodiabetes_samp[, -which(names(nodiabetes_samp) == \"Outcome\")])\n\n# Convert to dataframe for plotting\ndf_gower &lt;- as.data.frame(as.matrix(gower_dist)) %&gt;%\n  rownames_to_column('obs1') %&gt;%\n  pivot_longer(cols = -obs1, names_to = \"obs2\", values_to = \"gower\") %&gt;%\n  mutate(obs1 = as.numeric(obs1),\n         obs2 = as.numeric(obs2)) %&gt;%\n  filter(obs1 &lt; obs2)\n\n# Plot Gower distances\nggplot(df_gower, aes(x = gower)) +\n  geom_dotplot() +\n  labs(title = \"Gower Distances for Pairs of Survived Patients\",\n       x = \"Gower Distance\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis part calculates the Gower distance for pairs of patients who were classified as “No Diabetes” and visualizes the distances. The left cluster (0–100 Gower Distance) on the plot is showing most pairs have low Gower distances, meaning that many “No Diabetes” patients are quite similar to each other across all measured features. These similarities might be in features like glucose, age, BMI. It could indicate a consistent pattern in “No Diabetes” patients. The right Cluster (300–500 Gower Distance) is showing that some patient pairs have high Gower distances, suggesting they are much less similar. This indicates that within ‘No Diabetes’ patients, there is still heterogeneity, some people look very different based on their profile, yet still end up being classified as “No Diabetes”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "5.1 Tree Classification\nThe Decision Tree analysis offers valuable insights into the diagnostic factors affecting diabetes outcomes. Through simple and interpretable rules, it highlights Glucose as the dominant predictor while also acknowledging the role of other variables. This supports the project’s objective of clarifying significant factors and uncovering meaningful patterns, which ultimately helps guide interventions and educational efforts focused on diabetes prevention and monitoring.\nThe model performs reasonably well overall, with good accuracy and specificity, meaning it is effective at identifying patients without diabetes. However, the sensitivity for diabetes cases is moderate, meaning some true diabetes cases were missed. In a healthcare context, where failing to detect diabetes can have serious consequences, it may be beneficial to optimize for higher sensitivity even if that lowers overall accuracy slightly. The confusion matrix shows that false positives and false negatives are both present, but false negatives (missed diabetes) are slightly more concerning.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#random-forest",
    "href": "conclusion.html#random-forest",
    "title": "5  Conclusion",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\nWhile Random Forest did not significantly outperform the pruned Decision Tree in terms of accuracy, it achieved slightly better specificity and balanced accuracy, which indicates a more stable and robust performance. Random Forest still provides value in this analysis as it is less likely to overfit and better able to handle more complex or noisy data. It matches with the project’s goals, Random Forest complements Decision Tree analysis by supporting reliable diabetes detection in addition to interpretable decision making.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#partial-dependence-plot",
    "href": "conclusion.html#partial-dependence-plot",
    "title": "5  Conclusion",
    "section": "5.3 Partial Dependence Plot",
    "text": "5.3 Partial Dependence Plot\nRandom Forest provided high predictive performance, but we needed PDP to understand which variables matter and how they influence predictions. PDP revealed that Glucose, followed by BMI and DiabetesPedigreeFunction, are the strongest contributors to higher diabetes risk predictions. This method goes particularly well along with my project goal because now, we are not only predicting diabetes but also able to clarify which factors have significant impacts, offering deeper insights into diabetes outcomes that can help inform prevention strategies.\nThe variable importance method confirms the findings in PDP, the plot that variable importance provided have the same ranking aligns with the trends observed in the PDP plots, where Glucose and BMI showed more dramatic shifts in predicted probabilities. Together, these analyses confirm the central role of Glucose and BMI in predicting diabetes outcomes, reinforcing their importance in any decision making or medical guidance derived from the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#lime",
    "href": "conclusion.html#lime",
    "title": "5  Conclusion",
    "section": "5.4 LIME",
    "text": "5.4 LIME\nLIME offers individual-level explanations, showing which features contribute to each prediction individually. This complements PDP, it shows the global patterns and LIME shows the local, individualized explanations. LIME is helpful for decision support, especially when explaining to doctors or patients why a particular prediction was made. By breaking down complex predictions from models like Random Forest, LIME makes the decision making process clearer and easier.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#overall",
    "href": "conclusion.html#overall",
    "title": "5  Conclusion",
    "section": "5.5 Overall",
    "text": "5.5 Overall\nIn this project, we explored and compared several machine learning approaches to predict diabetes and interpret the results in meaningful ways. Starting with a Decision Tree, we quickly learned that while easy to interpret, unpruned trees risk overfitting and losing generalizability. Pruning improved performance and stability, but still left some room for improvement in predictive power.\nMoving to Random Forest allowed us to achieve more robust and balanced predictions. Although the overall accuracy improvement over the pruned Decision Tree was modest, Random Forest offered clear advantages in terms of reducing overfitting and handling more complex patterns in the data. This aligns well with our project goal of building a model that not only predicts accurately, but also generalizes reliably.\nHowever, Random Forest’s complexity makes it less interpretable. To address this, we incorporated Partial Dependence Plots, which revealed which revealed the variables with the strongest overall influence on predictions. Glucose, BMI, and DiabetesPedigreeFunction stood out as the most impactful features. The PDPs helped bridge the gap between predictive performance and interpretability at the global, overall model level.\nWe further confirmed these findings using Variable Importance plots, which supported the same variable rankings seen in the PDP analysis. This reinforced the understanding that Glucose and BMI are consistently crucial predictors across various perspectives.\nFinally, to address the need for local interpretability, we introduced LIME. Unlike PDP and Variable Importance, which show global trends, LIME allowed us to zoom in on individual predictions and understand exactly why a model predicted a particular outcome for a specific patient. This is especially useful for real world applications, case by case explanations before making decisions. LIME helps us to understand by turning complex model decisions into clear, straightforward explanations that make sense to nontechnical audiences\nOverall, with all these approaches combining predictive power with multiple forms of interpretability, allows to make accurate diabetes prediction and meaningful insights. Each method contributes uniquely, Decision Trees offer simplicity, Random Forest ensures robustness, PDP and Variable Importance offer global understanding, and LIME provides case-level clarity. Together, they form a comprehensive and practical framework for both prediction and explanation in diabetes detection and risk assessment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]