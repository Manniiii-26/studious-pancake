[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-goal",
    "href": "index.html#project-goal",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.1 Project Goal",
    "text": "1.1 Project Goal\nThe objective of my project is to draw inferences and predictions from patients’ diagnostic measurements related to the detection of diabetes, with a focus on uncovering patterns or relationships between variables. My main goal is to understand the interactions between the various factors that we have available and to clarify which factors have a significant impact on diabetes outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#significance-of-the-project-question",
    "href": "index.html#significance-of-the-project-question",
    "title": "Analysis of Diabetes with Machine Learning Methods",
    "section": "1.2 Significance of the Project Question",
    "text": "1.2 Significance of the Project Question\nThis topic is important to me because of the shocking statistics surrounding diabetes; approximately 38 million Americans have been diagnosed with diabetes, which is roughly 1 in 10 people. In this research effort, my goal is to investigate this issue in depth. I want to reveal the key factors that contribute to the development of diabetes, with the ultimate goal of educating people about diabetes prevention and encouraging them to be active in monitoring their health before the condition gets worse.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Context & Background Information\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The dataset has been collected with a sample size of 768 with 8 variables and a column of outcome. I got this dataset from Kaggle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#content",
    "href": "data.html#content",
    "title": "2  Data",
    "section": "2.2 Content",
    "text": "2.2 Content\n\nNumber of Sample: 768\nNumber of Attribute: 8\nPregnancies: number of times pregnant\nGlucose: plasma glucose concentration 2 hours in an oral glucose tolerance test\nBloodPressure: diastolic blood pressure (mm Hg)\nSkinThinkness: triceps skin fold thickness (mm)\nInsulin: 2-Hour serum insulin (mu U/ml)\nBMI: body mass index (weight in kg/(height in m)^2)\nDiabetesPedigreeFunction: diabetes pedigree function\nAge: age (years)\nOutcome: class variable (0 = No, 1 = Yes)\n\n\n\nCode\nlibrary(dplyr)\n\ndf &lt;- read.csv(\"data/diabetes.csv\")\n\nsummary(df)\n\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nCode\n# View summary of zeros\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        5                       35 \n           SkinThickness                  Insulin                      BMI \n                     227                      374                       11 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.3 Missing value analysis",
    "text": "2.3 Missing value analysis\nThis dataset have missing values, they uses ‘0’ to fill in the dataset beforhand so I am not able to tell rather the ‘0’ means missing or the real response is actually zero. Expect for the ‘Outcome’ variable, ‘0’ indicate no diabeties.\nFrom the summary, variable ‘Pregnancies’ have 111 responses are ‘0’ which I will not mutate this column. Since this dataset is all females at least 21 years old, I do believe that some of the zeros for pregnanices does mean no pregnanices history.\nThere is 5 missing values for ‘Glucose’, 35 missing values for ‘BloodPressure’, 277 missing values for ‘SkinThickness’, 374 missing values for ‘Insulin’, 11 missing values for ‘BMI’, and no missing values for ‘DiabetesPedigreeFunction’ and ‘Age’. I would replace zeros with the median of the non-zero values, median is robust to outliers and is commonly used in medical datasets.\n\n\nCode\n# Columns where 0 is considered missing (except 'Pregnancies' and 'Outcome')\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median of non-zero values in the specified columns\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n\n# Check result\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        0                        0 \n           SkinThickness                  Insulin                      BMI \n                       0                        0                        0 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500 \n\n\nIn my study, the variable I propose to categorize and make inferences about “outcome”, indicates the potential diagnosis of diabetes. This variable determines whether a diabetes test result is positive (category value 1) or negative (category value 0). A remarkable aspect of our dataset is the “pregnancy” variable, with a minimum value of 0, a maximum value of 17, and a mean value of 3.845. It would be particularly informative to explore whether pregnancy is an important factor influencing the outcome of a diabetes diagnosis and to assess the degree to which it does so. In addition, common predictors such as body mass index (BMI) and age are expected to play an important role in determining outcomes.\nFrom the summary, variable ‘Pregnancies’ have 111 responses are ‘0’ which I will not mutate this column. Since this dataset is all females at least 21 years old, I do believe that some of the zeros for pregnanices does mean no pregnanices history.\nThere is 5 missing values for ‘Glucose’, 35 missing values for ‘BloodPressure’, 277 missing values for ‘SkinThickness’, 374 missing values for ‘Insulin’, 11 missing values for ‘BMI’, and no missing values for ‘DiabetesPedigreeFunction’ and ‘Age’. I would replace zeros with the median of the non-zero values, median is robust to outliers and is commonly used in medical datasets.\n\n\nCode\n# Columns where 0 is considered missing (except 'Pregnancies' and 'Outcome')\nmissing_cols &lt;- c(\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\")\n\n# Replace 0 with median of non-zero values in the specified columns\nfor (col in missing_cols) {\n  median_value &lt;- median(df[[col]][df[[col]] != 0], na.rm = TRUE)\n  df[[col]][df[[col]] == 0] &lt;- median_value\n}\n\n# Check result\nsapply(df, function(x) sum(x == 0))\n\n\n             Pregnancies                  Glucose            BloodPressure \n                     111                        0                        0 \n           SkinThickness                  Insulin                      BMI \n                       0                        0                        0 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                      500 \n\n\nIn my study, the variable I propose to categorize and make inferences about “outcome”, indicates the potential diagnosis of diabetes. This variable determines whether a diabetes test result is positive (category value 1) or negative (category value 0). A remarkable aspect of our dataset is the “pregnancy” variable, with a minimum value of 0, a maximum value of 17, and a mean value of 3.845. It would be particularly informative to explore whether pregnancy is an important factor influencing the outcome of a diabetes diagnosis and to assess the degree to which it does so. In addition, common predictors such as body mass index (BMI) and age are expected to play an important role in determining outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "3.1 Statistical Technique",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#statistical-technique",
    "href": "methods.html#statistical-technique",
    "title": "3  Methods",
    "section": "",
    "text": "3.1.1 Tree Classification\n\nThe idea behind trees is to create multiple regions corresponding to different values (or intervals) of the predictors. Prediction is then conducted by assigning to a new observation the mean/median/mode (or maximum class probability) of the response computed over the region to which the new observation belongs to. Tree classification will also give us a meaningful visual representation of the variables.\n\n\n\n3.1.2 Random Forest\n\nRandom Forest, registered as a trademark by Leo Breiman and Adele Cutler. The algorithm’s ease of use and flexibility have contributed to its adoption since it can handle both classification and regression problems. It can help in identifying which features or variables are most important in making predictions. This is particularly useful for understanding the underlying patterns in the data and it also helps reduce the variance and gain uncorrelated trees.\n\n\n\n3.1.3 LIME\n\nLocal Interpretable Model-agnostic Explanations, starts by choosing a specific sample, x’, which is our data point of interest. It then perturbs this sample by creating fake data points in the neighborhood of x’. These fake data points are fed into the black box model to obtain predictions. Each fake data point is weighted based on its distance or similarity to x’, giving more importance to those closer to the original sample. After this, LIME fits a simple and interpretable model to these weighted fake data points. This local model is tuned as necessary and helps us observe how the black box model’s predictions change in response to variations around x’. Ultimately, this process reveals which features were most important for that specific prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "4  Results",
    "section": "",
    "text": "5 Partial Dependence Plot\nCode\n# Train Random Forest model\nset.seed(5293)\nrf_model &lt;- randomForest(Outcome ~ ., data = df)\n# Define variable names (independent variables)\nvars &lt;- colnames(df)[colnames(df) != \"Outcome\"]\n\n# Create PDP for each variable, combine into tidy dataframe\npdp_data &lt;- map_dfr(vars, function(v) {\n  pd &lt;- pdp::partial(rf_model, pred.var = v, prob = TRUE, grid.resolution = 20) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(variable = v)\n  # rename value and yhat columns to be consistent\n  names(pd)[1:2] &lt;- c(\"value\", \"yhat\")\n  return(pd)\n})\n\n# Plot using ggplot2 and facet_wrap\nggplot(pdp_data, aes(x = value, y = yhat)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~variable, scales = \"free_x\") +\n  labs(title = \"Partial Dependence Plots for All Variables\",\n       x = \"Value\", y = \"Predicted Probability (yhat)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#variable-importance",
    "href": "results.html#variable-importance",
    "title": "4  Results",
    "section": "5.1 Variable Importance",
    "text": "5.1 Variable Importance\n\n\nCode\n# Convert Outcome to factor\ndf$Outcome &lt;- as.factor(df$Outcome)\n\n# Train Random Forest (no formula interface, better for pdp and importance)\nset.seed(123)\nrf_model &lt;- randomForest(x = df[, -which(names(df) == \"Outcome\")],\n                         y = df$Outcome,\n                         importance = TRUE)\nrf_model$importance\n\n\n                             Diabetes No Diabetes MeanDecreaseAccuracy\nPregnancies               0.001932196 0.021298121          0.014328761\nGlucose                   0.091108804 0.056233815          0.068244656\nBloodPressure            -0.001466423 0.004508204          0.002373841\nSkinThickness             0.002604070 0.005950999          0.004594722\nInsulin                   0.002378496 0.009570177          0.006943391\nBMI                       0.041311386 0.021554611          0.028308705\nDiabetesPedigreeFunction  0.007456140 0.008793882          0.008246788\nAge                       0.022576763 0.028753605          0.026369235\n                         MeanDecreaseGini\nPregnancies                      28.24034\nGlucose                          90.30920\nBloodPressure                    30.92474\nSkinThickness                    23.45691\nInsulin                          24.80813\nBMI                              57.46094\nDiabetesPedigreeFunction         43.25725\nAge                              46.40131\n\n\nCode\n# Get importance\nimp &lt;- importance(rf_model, type = 2)  # type=2 is Mean Decrease Gini (recommended)\n\n# Convert to dataframe\nimp_df &lt;- data.frame(\n  variable = rownames(imp),\n  importance = imp[, 1]\n)\n\n# Scale importance to sum to 100\nimp_df &lt;- imp_df %&gt;%\n  mutate(importance_scaled = importance / sum(importance) * 100) %&gt;%\n  arrange(importance_scaled)\n\n# Plot\nggplot(imp_df, aes(x = importance_scaled, y = reorder(variable, importance_scaled))) +\n  geom_point(size = 3) +\n  labs(title = \"Variable Importance (Scaled to Sum to 100)\",\n       x = \"Variable Importance Score\",\n       y = \"\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nLIME\n\n\n\nCode\n# Split dataset into train and test\nset.seed(5293)\nn &lt;- nrow(df)\ntest_index &lt;- sample(n, 2)  # You can also use more test samples if you want\ntrain_data &lt;- df[-test_index, ]\ntest_data &lt;- df[test_index, ]\n\n\n# Train Random Forest\nrf_model &lt;- randomForest(Outcome ~ ., data = train_data)\n\n# Tell lime what type of model this is\nmodel_type.randomForest &lt;- function(x, ...) {\n  return(\"classification\")\n}\n\n# Prepare explainer using training data (remove Outcome column)\nexplainer &lt;- lime::lime(subset(train_data, select = -Outcome), rf_model)\n\n# Explain prediction for test_data\nexplanation &lt;- lime::explain(x = subset(test_data, select = -Outcome),\n                             explainer = explainer, labels = 'No Diabetes', n_features = 7,\n                             n_permutations = 1000, feature_select = 'lasso_path')\n\n# View explanation\nprint(explanation)\n\n\n# A tibble: 14 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 2 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 3 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 4 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 5 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 6 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 7 classificat… 339   No D…      0.156    0.433           0.746            0.151\n 8 classificat… 11    No D…      0.856    0.194           0.621            0.573\n 9 classificat… 11    No D…      0.856    0.194           0.621            0.573\n10 classificat… 11    No D…      0.856    0.194           0.621            0.573\n11 classificat… 11    No D…      0.856    0.194           0.621            0.573\n12 classificat… 11    No D…      0.856    0.194           0.621            0.573\n13 classificat… 11    No D…      0.856    0.194           0.621            0.573\n14 classificat… 11    No D…      0.856    0.194           0.621            0.573\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\nCode\n# Optional: visualize explanation\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Explain prediction for test_data\nexplanation2 &lt;- lime::explain(x = subset(test_data, select = -Outcome),\n                             explainer = explainer, labels = 'Diabetes', n_features = 7,\n                             n_permutations = 1000, feature_select = 'lasso_path')\n\n# View explanation\nprint(explanation2)\n\n\n# A tibble: 14 × 13\n   model_type   case  label label_prob model_r2 model_intercept model_prediction\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 2 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 3 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 4 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 5 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 6 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 7 classificat… 339   Diab…      0.844    0.411           0.272            0.794\n 8 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n 9 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n10 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n11 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n12 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n13 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n14 classificat… 11    Diab…      0.144    0.202           0.397            0.417\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\nCode\n# Optional: visualize explanation\nplot_features(explanation2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for only 'survived' patients\ndiabetes &lt;- df %&gt;%\n  filter(Outcome == \"Diabetes\")\n\n# Randomly sample 10 survived patients\nset.seed(123)\nindex &lt;- sample(nrow(diabetes), 10)\ndiabetes_samp &lt;- diabetes[index, ]\n\n# Compute Gower distance matrix (excluding Outcome column)\ngower_dist &lt;- daisy(diabetes_samp[, -which(names(diabetes_samp) == \"Outcome\")])\n\n# Convert to dataframe for plotting\ndf_gower &lt;- as.data.frame(as.matrix(gower_dist)) %&gt;%\n  rownames_to_column('obs1') %&gt;%\n  pivot_longer(cols = -obs1, names_to = \"obs2\", values_to = \"gower\") %&gt;%\n  mutate(obs1 = as.numeric(obs1),\n         obs2 = as.numeric(obs2)) %&gt;%\n  filter(obs1 &lt; obs2)\n\n# Plot Gower distances\nggplot(df_gower, aes(x = gower)) +\n  geom_dotplot() +\n  labs(title = \"Gower Distances for Pairs of Diabetes Patients\",\n       x = \"Gower Distance\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnodiabetes &lt;- df %&gt;%\n  filter(Outcome == \"No Diabetes\")\n\nset.seed(123)\nindex &lt;- sample(nrow(nodiabetes), 10)\nnodiabetes_samp &lt;- nodiabetes[index, ]\n\n# Compute Gower distance matrix (excluding Outcome column)\ngower_dist &lt;- daisy(nodiabetes_samp[, -which(names(nodiabetes_samp) == \"Outcome\")])\n\n# Convert to dataframe for plotting\ndf_gower &lt;- as.data.frame(as.matrix(gower_dist)) %&gt;%\n  rownames_to_column('obs1') %&gt;%\n  pivot_longer(cols = -obs1, names_to = \"obs2\", values_to = \"gower\") %&gt;%\n  mutate(obs1 = as.numeric(obs1),\n         obs2 = as.numeric(obs2)) %&gt;%\n  filter(obs1 &lt; obs2)\n\n# Plot Gower distances\nggplot(df_gower, aes(x = gower)) +\n  geom_dotplot() +\n  labs(title = \"Gower Distances for Pairs of Survived Patients\",\n       x = \"Gower Distance\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; fa690e0a2a368b101ba088cd3afcaa399e913d92",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  }
]